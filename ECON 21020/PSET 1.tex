\documentclass[11pt]{article}

% NOTE: Add in the relevant information to the commands below; or, if you'll be using the same information frequently, add these commands at the top of paolo-pset.tex file. 
\newcommand{\name}{Agustín Esteva}
\newcommand{\email}{aesteva@uchicago.edu}
\newcommand{\classnum}{20210}
\newcommand{\subject}{Econometric}
\newcommand{\instructors}{Murilo Ramos}
\newcommand{\assignment}{Problem Set 1}
\newcommand{\semester}{Summer 2025}
\newcommand{\duedate}{\today}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bD}{\mathbf{D}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bG}{\mathbf{G}}
\newcommand{\bH}{\mathbf{H}}
\newcommand{\bI}{\mathbf{I}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\bK}{\mathbf{K}}
\newcommand{\bL}{\mathbf{L}}
\newcommand{\bM}{\mathbf{M}}
\newcommand{\bN}{\mathbf{N}}
\newcommand{\bO}{\mathbf{O}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bS}{\mathbf{S}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\Vol}{\text{Vol}}
\newcommand{\MSE}{\text{MSE}}
\newcommand{\Bias}{\text{Bias}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Corr}{\text{Corr}}



%% blackboard bold math capitals
\newcommand{\bbA}{\mathbb{A}}
\newcommand{\bbB}{\mathbb{B}}
\newcommand{\bbC}{\mathbb{C}}
\newcommand{\bbD}{\mathbb{D}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\bbF}{\mathbb{F}}
\newcommand{\bbG}{\mathbb{G}}
\newcommand{\bbH}{\mathbb{H}}
\newcommand{\bbI}{\mathbb{I}}
\newcommand{\bbJ}{\mathbb{J}}
\newcommand{\bbK}{\mathbb{K}}
\newcommand{\bbL}{\mathbb{L}}
\newcommand{\bbM}{\mathbb{M}}
\newcommand{\bbN}{\mathbb{N}}
\newcommand{\bbO}{\mathbb{O}}
\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbQ}{\mathbb{Q}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\bbS}{\mathbb{S}}
\newcommand{\bbT}{\mathbb{T}}
\newcommand{\bbU}{\mathbb{U}}
\newcommand{\bbV}{\mathbb{V}}
\newcommand{\bbW}{\mathbb{W}}
\newcommand{\bbX}{\mathbb{X}}
\newcommand{\bbY}{\mathbb{Y}}
\newcommand{\bbZ}{\mathbb{Z}}

%% script math capitals
\newcommand{\sA}{\mathscr{A}}
\newcommand{\sB}{\mathscr{B}}
\newcommand{\sC}{\mathscr{C}}
\newcommand{\sD}{\mathscr{D}}
\newcommand{\sE}{\mathscr{E}}
\newcommand{\sF}{\mathscr{F}}
\newcommand{\sG}{\mathscr{G}}
\newcommand{\sH}{\mathscr{H}}
\newcommand{\sI}{\mathscr{I}}
\newcommand{\sJ}{\mathscr{J}}
\newcommand{\sK}{\mathscr{K}}
\newcommand{\sL}{\mathscr{L}}
\newcommand{\sM}{\mathscr{M}}
\newcommand{\sN}{\mathscr{N}}
\newcommand{\sO}{\mathscr{O}}
\newcommand{\sP}{\mathscr{P}}
\newcommand{\sQ}{\mathscr{Q}}
\newcommand{\sR}{\mathscr{R}}
\newcommand{\sS}{\mathscr{S}}
\newcommand{\sT}{\mathscr{T}}
\newcommand{\sU}{\mathscr{U}}
\newcommand{\sV}{\mathscr{V}}
\newcommand{\sW}{\mathscr{W}}
\newcommand{\sX}{\mathscr{X}}
\newcommand{\sY}{\mathscr{Y}}
\newcommand{\sZ}{\mathscr{Z}}


\renewcommand{\emptyset}{\O}

\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\sm}{\setminus}


\newcommand{\sarr}{\rightarrow}
\newcommand{\arr}{\longrightarrow}

% NOTE: Defining collaborators is optional; to not list collaborators, comment out the line below.
%\newcommand{\collaborators}{Alyssa P. Hacker (\texttt{aphacker}), Ben Bitdiddle (\texttt{bitdiddle})}

\input{paolo-pset.tex}

% NOTE: To compile a version of this pset without problems, solutions, or reflections, uncomment the relevant line below.

%\excludeversion{problem}
%\excludeversion{solution}
%\excludeversion{reflection}

\begin{document}	
	
	% Use the \psetheader command at the beginning of a pset. 
	\psetheader

\section*{Problem 1}
Suppose $X\sim\text{Bernoulli}(p).$ 
\begin{enumerate}
    \item Show $\bbE[X^3] = p$
\begin{solution}
Using the following part, we let $k = 3$ and conclude
\end{solution}
\item Show $\bbE[X^k]= p$
\begin{solution}
    By definition, 
    \begin{align*}
        \bbE[X^k] &= 1^k \bbP\{X = 1\} + 0^k\bbP\{X = 0\}\\
        &= \boxed{p}
    \end{align*}
\end{solution}
\item Suppose that p = 0.3. Compute the mean, variance, skewness, and kurtosis of X.
\begin{solution}
    From previous part, $\bbE[X] = p = \boxed{0.3}.$ $\Var[X] = p(1-p) = \boxed{0.21}.$ To calculate the skewness, we compute 
    \begin{align*}
        \bbE\left[\left(\frac{X - \bbE[X]}{\sqrt{\bbV[X]}}\right)^3\right] &= \frac{1}{\bbV[X]^\frac{3}{2}}\bbE[(X - \mu)^3]\\
        &= \frac{1}{\bbV[X]^\frac{3}{2}}(\bbE[X^3] - 3\bbE[X^2]\bbE[X] + 2\bbE[X]^3\\
        &= \frac{p - 3p^2 + 2p^3}{(p(1-p))^\frac{3}{2}}\\
        &= \frac{p(1-p)(1-2p)}{(p(1-p))^\frac{3}{2}}\\
        &= \frac{1-2p}{\sqrt{1-p}}\\
        &\approx \boxed{0.87}
    \end{align*}
    Using similar algebra, we skip a few steps:
    \[\bbE\left[\left(\frac{X - \bbE[X]}{\sqrt{\bbV[X]}}\right)^4\right] = \frac{1-6p(1-p)}{p(1-p)} +3= \boxed{1.762}\]
\end{solution}
\end{enumerate}
In a population, $\mu_Y = 100$ and $\sigma_Y^2 = 43$. Use the Central Limit Theorem to answer the following questions:

\begin{enumerate}
    \item[(a)] In a random sample of size $n = 100$, find 
    \[
    \Pr\left(\overline{Y} \leq 101\right).
    \]
\begin{solution}
    We know by the CLT
    \[\frac{\sqrt{100}(\overline{Y} - 100)}{\sqrt{43}} \sim N(0,1)\] Hence 
    \[\bbP\{\overline{Y} \leq 101\} = \bbP\{\frac{10(\overline{Y} - 100)}{\sqrt{43}} \leq\frac{10(101 - 100)}{\sqrt{43}}\} = \bbP\{Z \leq \frac{10}{\sqrt{43}}\} \approx \boxed{0.94}\]
\end{solution}

    \item[(b)] In a random sample of size $n = 165$, find 
    \[
    \Pr\left(\overline{Y} > 98\right).
    \]
\begin{solution}
    Using similar logic to the above problem, we find that 
    \[Z = \frac{\sqrt{165}(\overline{Y} - 100)}{\sqrt{43}} \sim N(0,1)\] so then 
    \[\bbP\{Z \geq  \frac{\sqrt{165}(98 - 100)}{\sqrt{43}}\}\approx \boxed{1}\]

\end{solution}

    \item[(c)] In a random sample of size $n = 64$, find 
    \[
    \Pr\left(101 \leq \overline{Y} \leq 103\right).
    \]
    \begin{solution}
\[Z = \frac{8(\overline{Y} - 100)}{\sqrt{43}}\sim N(0,1)\] and thus 
\[\bbP\{\frac{8(101 - 100)}{\sqrt{43}} \leq Z \leq \frac{8(103 - 100)}{\sqrt{43}}\}
 = \boxed{0.1111}\]
    \end{solution}
\end{enumerate}


\newpage
\section*{Problem 2}
\begin{problem}
    Show that $\bbE[Y\mid X]$ is minimizes 
    \[\min_{g(X)} \bbE[(Y - g(X))^2]\]
\end{problem}
\begin{solution}
We claim that $Y - \bbE[Y \mid X] \perp \bbE[Y \mid X] - g(X).$ To see this, we note that 
\begin{align*}
 \langle Y - \bbE[Y \mid X], \bbE[Y \mid X] - g(X)\rangle &= \langle Y, \bbE[Y \mid X]\rangle - \langle Y, g(X)\rangle -\langle \bbE[Y \mid X], \bbE[Y \mid X] \rangle + \langle \bbE[Y \mid X], g(X)\rangle\\
 &= \bbE[Y\bbE[Y \mid X]] - \bbE[Y g(X)] - \bbE[\bbE[Y \mid X]^2] + \bbE[\bbE[Y \mid X]g(X)]\\
 &= \bbE[\bbE[Y \bbE[Y \mid X] \mid X]] - \bbE[\bbE[Y g(X) \mid X]] - \bbE[\bbE[Y \mid X]^2] + \bbE[\bbE[Y \mid X]g(X)]\\
 &= \bbE[\bbE[Y \mid X]^2] - \bbE[g(X) \bbE[Y \mid X]] - \bbE[\bbE[Y \mid X]^2] +  \bbE[g(X)\bbE[Y \mid X]]\\
 &= 0
\end{align*}
Here, we use the fact that $g(X)$ is $X-$measurable, and thus we pull it out of the conditional expectation. We also made heavy use of LIE. By orthogonality, we can use the Pythagorean theorem


Computing, 
\begin{align*}
    \bbE[(Y - g(X))^2] &= \bbE[(Y - \bbE[Y \mid X]+ \bbE[Y \mid X] - g(X))^2]\\
    &= \|(Y - \bbE[Y \mid X])+ (\bbE[Y \mid X] - g(X))\|^2\\
    &= \|Y - \bbE[Y \mid X]\|^2 + \|\bbE[Y \mid X] - g(X)\|^2
\end{align*}
Clearly, the left hand side will be minimized when \[\|\bbE[Y \mid X] - g(X)\|^2 = 0\iff \bbE[Y \mid X] - g(X) = 0 \iff g(X) = \bbE[Y \mid X]\]
\end{solution}

\newpage
\section*{Problem 3}
\begin{problem}
    Prove that 
    \[\Cov(X,Y) = \bbE[XY] - \bbE[X]\bbE[Y]\]
\end{problem}
\begin{solution}
    By definition,
    \begin{align*}
        \Cov(X,Y) &\equiv \bbE[(X - \bbE[X])(Y - \bbE[Y])]\\
        &= \bbE[XY - Y\bbE[X] - X\bbE[Y] + \bbE[X]\bbE[Y]\\
        &= \bbE[XY] - \bbE[Y \bbE[X]] - \bbE[X \bbE[Y]] + \bbE[\bbE[X]\bbE[Y]]\\
        &= \bbE[XY] - \bbE[Y]\bbE[X] + (-\bbE[X]\bbE[Y] + \bbE[X]\bbE[Y])\\
        &= \bbE[XY] - \bbE[Y]\bbE[X]
    \end{align*}
    Here we make heavy use of the fact that $\bbE[Z]$ is a constant and can thus be pulled out of the expectation.
\end{solution}

\newpage
\section*{Problem 4}
\begin{problem}
    Suppose that in the State of Illinois the written exam for a drivers license consists of 10 multiple-choice
 questions. Each question has 4 possible choices, only one of which is correct. Passing requires answering
 at least 5 questions correctly. What is the probability a student-driver passes his exam by “guessing
 randomly” on each question?
\end{problem}
\begin{solution}
    Let $X \sim \text{Binomial}(10, \frac{1}{4}).$ It should be clear that the solution to this problem is the same as finding 
\begin{align*}
    \bbP[X \geq 5] &= 1 - (\sum_{k=0}^4 \binom{10}{k}(\frac{1}{4})^k(\frac{3}{4})^{10-k}) = \boxed{0.0781}
\end{align*}
where the numerical answer was found using software.

\end{solution}

\newpage
\section*{Problem 5}
\begin{problem}
    Let $X \sim \text{Bernoulli}(p).$ Define $Z = 3^X -1.$ 
\begin{itemize}
    \item Is $Z$ a random variable. Why?
    \begin{solution}
        Yes, functions of random variabls are clearly random variables. To be precise, we assum that $(\Omega, \mathcal{F}, \bbP)$ is probability space, then $X: \Omega \to \bbR$ is $\mathcal{F}$ measurable. Since $Y = g(X)$ where $g$ is continuous, then clearly $g$ is $\cal F$ measurable and thus a random variable.
    \end{solution}
    \item Show that $\bbE[Z] = 2p.$
    \begin{solution}
        \begin{align*}
            \bbE[Z] &= \bbE[3^X -1]\\
            &= \bbE[3^X] -1\\
            &= 3^1 p + 3^0 (1-p) - 1\\
            &= 3p + 1 - p -1 \\
            &= 1+2p -1 \\
            &= \boxed{2p}
        \end{align*}
    \end{solution}
    \item Show that $\bbE[Z^2] = 4p$
    \begin{solution}
    We use work from the previous part to find that
        \begin{align*}
            \bbE[Z^2] &= \bbE[(3^X -1)^2]\\
            &= \bbE[3^{2X} - 2\cdot 3^X + 1]\\
            &= \bbE[3^{2X}] - 2\bbE[3^X] +1\\
            &= 1+2p- 2(1+2p) + 1\\
            &= \boxed{4p}
        \end{align*}
    \end{solution}
    \item 
    Find $\Var(Z)$
    \begin{solution}
        We use the identity and the previous problems to find that
        \[\Var(Z) = \bbE[Z^2] - \bbE[Z]^2 = \boxed{4p - 4p^2}\]
    \end{solution}
\end{itemize}
\end{problem}

\newpage
\section*{Problem 6}
\begin{problem}
    Let \texttt{GPA} denote a random variable for a college student’s grade point average, and \texttt{SAT} denote a
random variable for the college student’s SAT score. Suppose that there is the following relationship
between \texttt{GPA} and \texttt{SAT}:
\[
\mathbb{E}[\texttt{GPA} \mid \texttt{SAT}] = 0.70 + 0.002 \cdot \texttt{SAT}.
\]

\begin{enumerate}
    \item[(a)] What is the expected GPA when \texttt{SAT} = 750? What is the expected GPA when \texttt{SAT} = 1500?
\begin{solution}
    Plugging in, 
\[\bbE[\texttt{GPA} \mid 750] = 0.70 + 0.002 \cdot 750 = \boxed{2.20}\]
\[\bbE[\texttt{GPA} \mid 1500] = 0.70 + 0.002 \cdot 1500 = \boxed{3.70}\]
\end{solution}
    \item[(b)] If \( \mathbb{E}[\texttt{SAT}] = 1000 \), what is \( \mathbb{E}[\texttt{GPA}] \)?
\begin{solution}
    Using the Law of Total Expectation:
    \[\bbE[\texttt{GPA}] = \bbE[\bbE[\texttt{GPA}\mid \texttt{SAT}] = \bbE[0.70 + 0.002 \cdot \texttt{SAT}] = 0.70 + 0.002\bbE[\texttt{SAT}] = \boxed{2.70}\]
\end{solution}
\end{enumerate}

\end{problem}


\newpage
\section*{Problem 7}
\begin{problem}
    Suppose \( X \sim \text{Unif}(-1,1) \), and let \( Y = X^2 \). Show that \( \operatorname{Cov}[X,Y] = 0 \), but that \( X \) and \( Y \) are not independent.
\end{problem}
\begin{solution}
    By Problem 3,
\[\Cov(X,Y) = \bbE[XY] - \bbE[X]\bbE[Y] = \bbE[X^3] - \bbE[X]\bbE[X^2]\] We compute, noting that the pdf of $X$ is 
\[f_X(x) = \frac{1}{2}\]
\[\bbE[X] = \frac{-1 + 1}{2} = 0\]
\[\bbE[X^2] = \int_{\bbR} X^2\,d\bbP = \int_{-1}^1 \frac{1}{2}x^2 \,dx =  \frac{1}{3}\]
\[\bbE[X^3] = \int_{\bbR} X^3\,d\bbP = \int_{-1}^1 \frac{1}{2}x^3 \,dx = \frac{1}{4}x^4 \bigg|_{-1}^1 = \frac{1}{4} - \frac{1}{4} = 0\]

Hence, the covariance is zero. To see that $X$ and $Y$ are not independent, simply compare the joint densities. We know that $X$ and $Y$ are independent iff
\begin{align}
f_{X,Y}(x,y) = f_X(x)f_Y(y)    
\end{align}

We know that $f_X(x) = \frac{1}{2}.$ Moreover, supposing that $y \in (-1,1),$
\[F_Y(y) = \bbP\{Y \leq y\} = \bbP\{X^2 \leq y\} = \bbP\{-\sqrt{y} \leq X \leq \sqrt{y}\} = \bbP\{X \leq \sqrt{y}\} - \bbP\{X \leq -\sqrt{y}\} =
\sqrt{y}.\] Hence, $f_Y(y) = \frac{1}{2\sqrt{y}}.$ Thus the right hand side of (1) is $\frac{1}{4\sqrt{y}}.$ To compute the left hand side, first we compute the conditional density 
\[f_{Y \mid X}(y)\] 
\end{solution}

\newpage
\section*{Problem 8}
\begin{problem}
    Let \( X \) be a random variable such that
\[
\mathbb{P}(X = -1) = \mathbb{P}(X = 0) = \mathbb{P}(X = 1) = \frac{1}{3}.
\]

Let \( Y \) be another random variable such that
\[
\mathbb{E}[Y \mid X] = 3 + 3X \quad \text{and} \quad \operatorname{Var}(Y \mid X) = 3.
\]
For each question below, provide a numerical answer and show your work.

\begin{enumerate}[label=(\alph*)]
    \item What is \( \mathbb{E}[X] \)?
\begin{solution}
    \[\bbE[X] = -1 \cdot \frac{1}{3} + 0 \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} = \boxed{0}\]
\end{solution}
    \item What is \( \mathbb{E}[X^2] \)?
\begin{solution}
    \[\bbE[X^2] = 2(1\cdot \frac{1}{3}) + 0 \cdot \frac{1}{3} = \boxed{\frac{2}{3}}\]
\end{solution}
    \item What is \( \mathbb{E}[Y] \)?
    \begin{solution}
        
\[\bbE[Y] = \bbE[\bbE[Y \mid X]] = \bbE[3 + 3X]] = 3 + 3\bbE[X] = \boxed{3}\]
    \end{solution}

    \item What is \( \operatorname{Var}[2 + \mathbb{E}[X]] \)?
    \begin{solution}
        Bro what the variance of a number is just zero.
    \end{solution}
    \item What is \( \operatorname{Var}[X] \)?
    \begin{solution}
        \[\Var(X) = \bbE[X^2] - \bbE[X]^2 =\boxed{ \frac{2}{3}}\]
    \end{solution}
    \item What is \( \operatorname{Var}[Y] \)?
    \begin{solution}
\begin{align*}
\Var(Y) &= \bbE[\Var(Y \mid X)] + \Var(\bbE[Y \mid X])    \\
&= \bbE[3] + \Var(3 + 3X)\\
&= 3 + 9\cdot\Var(X)\\
&= \boxed{9}
\end{align*}
    \end{solution}
    \item What is \( \operatorname{Cov}[X, Y] \)?
\begin{solution}
\begin{align*}
\Cov(X,Y) &= \bbE[XY] - \bbE[X] \bbE[Y] \\
&= \bbE[3X + 3X^2] - \bbE[X]\bbE[Y]\\
&=  3\bbE[X^2] \\
&= \boxed{2}
\end{align*}

\end{solution}
    \item What is \( \operatorname{Corr}[X, Y] \)?
    \begin{solution}
\[\Corr(X,Y) = \frac{\Cov(X,Y)}{\sqrt{\Var(X)}\sqrt{\Var(Y)}} = \frac{2}{\sqrt{6}} \]
    \end{solution}
    \item Is \( Y \) mean independent of \( X \)? Explain briefly.
\begin{solution}
Nah. Assume, for the sake of contradiction, that it is mean independent. Then $\Corr(X,Y) = 0.$ This is a contradiction to part $h$.
\end{solution}
    \item Is \( Y \) independent of \( X \)? Explain briefly.
\begin{solution}
    Nah, same reason as part i.
\end{solution}
\end{enumerate}
\end{problem}


\end{document}